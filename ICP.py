# -*- coding: utf-8 -*-
"""ICP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/177OzVPYHG12-NTJvURUfX4lx4dCHdMw3
"""

from __future__ import print_function
import numpy as np
import os
import glob
import numpy as np
import skimage.io as io
import skimage.transform as trans
from keras.models import *
from keras.layers import *
from keras.optimizers import *
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras import backend as keras
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
#from tensorflow.keras.layers.advanced_activations import LeakyReLU
from keras.layers import LeakyReLU
import tensorflow as tf
import warnings
from PIL import Image
from time import time
from tensorflow.keras.models import save_model  # Ensure you're using TensorFlow/Keras
from tensorflow.keras.models import load_model
from keras.layers import LeakyReLU

from google.colab import drive
drive.mount('/content/drive')

#path = '/private/Ombria_Data/OmbriaS2/train' # Sentinel 2 training data path
#path = '/content/drive/MyDrive/Forth/OMBRIA/OmbriaS2/train'
# Get user-specified directory
#save_dir = '/content/drive/MyDrive/Forth/OMBRIA/trained_models/S1_S2'

percentage = 80
s = 'S1'
epochs = 50


test_path = f'/content/drive/MyDrive/Forth/OMBRIA/Split_{percentage}/s1/reserved'
test_path1 = f'/content/drive/MyDrive/Forth/OMBRIA/Split_{percentage}/s2/reserved'
test_path2 = f'/content/drive/MyDrive/Forth/OMBRIA/Split_{percentage}/s1/reserved'

saved_model_path = f"/content/drive/MyDrive/Forth/OMBRIA/trained_models_split_{percentage}/{s}/FloodNet_model_{epochs}epochs.keras" # trained for 50 epochs

model = load_model(saved_model_path, custom_objects={'LeakyReLU': LeakyReLU})

"""# S1 / S2"""

cell_code_s1 = """
def testGenerator(test_path1):
  #path_before1 =test_path1 + '/AFTER'
  #path_after1 = test_path1 + '/BEFORE'
  path_before1 =test_path1 + '/A'
  path_after1 = test_path1 + '/B'
  filename1 = os.listdir(path_before1)
  filename1.sort()
  filename2 = os.listdir(path_after1)
  filename2.sort()
  for i in range(0,len(filename1)):
      img_before1 = io.imread(os.path.join(path_before1,filename1[i]))
      img_after1 = io.imread(os.path.join(path_after1,filename2[i]))

      # Convert from (256,256,3) --> (256,256,1) by keeping only one channel
      img_before1 = np.expand_dims(img_before1[:, :, 0], axis=-1)  # Select first channel and add dimension
      img_after1 = np.expand_dims(img_after1[:, :, 0], axis=-1)
      # **Squeeze last dimension to make it (1, 256, 256)**
      img_before1 = tf.squeeze(img_before1, axis=-1)
      img_after1 = tf.squeeze(img_after1, axis=-1)

      img_before1 = img_before1 / 255
      img_before1 = np.reshape(img_before1,(1,)+img_before1.shape)


      img_after1 = img_after1 / 255
      img_after1 = np.reshape(img_after1,(1,)+img_after1.shape)

      # Convert all to tf.float32 tensors
      img_before1 = tf.convert_to_tensor(img_before1, dtype=tf.float32)
      img_after1 = tf.convert_to_tensor(img_after1, dtype=tf.float32)

      yield ((img_before1, img_after1),)

# Define input signature for TensorFlow dataset
output_signature = ((
    #tf.TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32),  # img_before1
    #tf.TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32),  # img_after1
    tf.TensorSpec(shape=(1, 256, 256), dtype=tf.float32),  # img_before2
    tf.TensorSpec(shape=(1, 256, 256), dtype=tf.float32)   # img_after2
),
)

# Create a tf.data.Dataset from the generator
test_dataset = tf.data.Dataset.from_generator(
    lambda: testGenerator(test_path),
    output_signature=output_signature
)

# Use the dataset for prediction
results = model.predict(test_dataset, verbose=1)
"""

cell_code_s2 = """
def testGenerator(test_path1):
  path_before1 =test_path1 + '/AFTER'
  path_after1 = test_path1 + '/BEFORE'
  filename1 = os.listdir(path_before1)
  filename1.sort()
  filename2 = os.listdir(path_after1)
  filename2.sort()
  for i in range(0,len(filename1)):
      img_before1 = io.imread(os.path.join(path_before1,filename1[i]))
      img_after1 = io.imread(os.path.join(path_after1,filename2[i]))

      img_before1 = img_before1 / 255
      img_before1 = np.reshape(img_before1,(1,)+img_before1.shape)


      img_after1 = img_after1 / 255
      img_after1 = np.reshape(img_after1,(1,)+img_after1.shape)

      # Convert all to tf.float32 tensors
      img_before1 = tf.convert_to_tensor(img_before1, dtype=tf.float32)
      img_after1 = tf.convert_to_tensor(img_after1, dtype=tf.float32)

      yield ((img_before1, img_after1),)

# Define input signature for TensorFlow dataset
output_signature = ((
    tf.TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32),  # img_before1
    tf.TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32),  # img_after1
    #tf.TensorSpec(shape=(1, 256, 256), dtype=tf.float32),  # img_before2
    #tf.TensorSpec(shape=(1, 256, 256), dtype=tf.float32)   # img_after2
),
)

# Create a tf.data.Dataset from the generator
test_dataset = tf.data.Dataset.from_generator(
    lambda: testGenerator(test_path),
    output_signature=output_signature
)

# Use the dataset for prediction
results = model.predict(test_dataset, verbose=1)
"""

"""# S1 & S2"""

cell_code_s1_s2 = """
def testGenerator(test_path1,test_path2):
  path_before1 =test_path1 + '/AFTER'
  path_after1 = test_path1 + '/BEFORE'
  path_before2 =test_path2 + '/AFTER'
  path_after2 = test_path2 + '/BEFORE'
  filename1 = os.listdir(path_before1)
  filename1.sort()
  filename2 = os.listdir(path_after1)
  filename2.sort()
  filename3 = os.listdir(path_before2)
  filename3.sort()
  filename4 = os.listdir(path_after2)
  filename4.sort()
  for i in range(0,len(filename1)):
      img_before1 = io.imread(os.path.join(path_before1,filename1[i]))
      img_after1 = io.imread(os.path.join(path_after1,filename2[i]))

      img_before1 = img_before1 / 255
      img_before1 = np.reshape(img_before1,(1,)+img_before1.shape)


      img_after1 = img_after1 / 255
      img_after1 = np.reshape(img_after1,(1,)+img_after1.shape)



      img_before2 = io.imread(os.path.join(path_before2,filename3[i]))
      img_after2 = io.imread(os.path.join(path_after2,filename4[i]))

      img_before2 = img_before2 / 255
      img_before2 = np.reshape(img_before2,(1,)+img_before2.shape)


      img_after2 = img_after2 / 255
      img_after2 = np.reshape(img_after2,(1,)+img_after2.shape)

      # Convert all to tf.float32 tensors
      img_before1 = tf.convert_to_tensor(img_before1, dtype=tf.float32)
      img_after1 = tf.convert_to_tensor(img_after1, dtype=tf.float32)
      img_before2 = tf.convert_to_tensor(img_before2, dtype=tf.float32)
      img_after2 = tf.convert_to_tensor(img_after2, dtype=tf.float32)


      yield ((img_before1, img_after1, img_before2, img_after2),)

# Define input signature for TensorFlow dataset
output_signature = ((
    tf.TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32),  # img_before1
    tf.TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32),  # img_after1
    tf.TensorSpec(shape=(1, 256, 256), dtype=tf.float32),  # img_before2
    tf.TensorSpec(shape=(1, 256, 256), dtype=tf.float32)   # img_after2
),
)

# Create a tf.data.Dataset from the generator
test_dataset = tf.data.Dataset.from_generator(
    lambda: testGenerator(test_path1, test_path2),
    output_signature=output_signature
)


# Use the dataset for prediction
results = model.predict(test_dataset, verbose=1)
"""

"""# Rest"""

cell_code_rest = """
def load_images_from_folder(folder):
    images = []
    img_list = os.listdir(folder)
    img_list.sort()
    for filename in img_list:
        img = io.imread(os.path.join(folder,filename))
        img = img / 255
        images.append(img)
    return images

#y_true = load_images_from_folder(f"{test_path}/MASK")
#x1_true = load_images_from_folder(f"{test_path}/BEFORE")
#x2_true = load_images_from_folder(f"{test_path}/AFTER")
y_true = load_images_from_folder(f"{test_path}/label")

#y_true = [np.where(arr > 0, 1, 0) for arr in y_true]

"""

cell_code_TS1 = """
def inverse_sigmoid(probabilities, epsilon=1e-7):
    # Clip probabilities to avoid log(0) or log(1)
    probabilities = np.clip(probabilities, epsilon, 1 - epsilon)
    return np.log(probabilities / (1 - probabilities))

# Convert probabilities back to logits
logits = inverse_sigmoid(results)
"""

cell_code_TS2 = """
from scipy.optimize import minimize_scalar

def temperature_scaling(logits, T):
    # Apply temperature scaling to logits.
    return logits / T

def nll_loss(T, logits, labels):
    # Compute Negative Log-Likelihood (NLL) loss for a given temperature.
    scaled_logits = temperature_scaling(logits, T)
    probs = tf.sigmoid(scaled_logits)  # Use sigmoid for binary classification
    loss = tf.keras.losses.binary_crossentropy(labels, probs)  # Compute NLL
    return tf.reduce_mean(loss).numpy()  # Convert to scalar for optimization

def find_best_temperature(logits, labels):
    # Find the optimal temperature by minimizing NLL.
    result = minimize_scalar(lambda T: nll_loss(T, logits, labels), bounds=(0.01, 10), method='bounded')
    return result.x  # Optimal T
"""

cell_code_TS3 = """
logits_ = np.squeeze(logits,axis=-1)
labels = y_true

best_T = find_best_temperature(logits_, labels)
print(f"Optimal Temperature: {best_T:.4f}")
"""

cell_code_TS4 = """
T = best_T#2.1

# Convert back to probabilities
scaled_probabilities = tf.sigmoid(temperature_scaling(logits,T)).numpy()
"""

cell_code_CP = """
alpha = alpha_#0.1
# Obtain predictions from the model
n = np.shape(results)[0]*np.shape(results)[1]*np.shape(results)[2]
print(n)
#results = scaled_probabilities#model.predict(test_dataset, verbose=1)
y_pred = np.where(results[:, :, :, 0] > 0.5, 1, 0)

# Flatten predictions and ground truth
y_pred_vector = y_pred.flatten()
results_vector = results.reshape(n)
probability_array = np.column_stack((1 - results_vector, results_vector))
true_mask_vector = np.array(y_true).flatten().astype(int)

# Fixed calibration and validation splits
cal_pred = probability_array
cal_labels = true_mask_vector

# Conformal Prediction calculations
# Calculate conformal scores for the calibration set
cal_scores = 1 - cal_pred[np.arange(n), cal_labels]

# Calculate quantile and threshold
q_level = np.ceil((n + 1) * (1 - alpha)) / n
qhat = np.quantile(cal_scores, q_level, interpolation='higher')


# Define output filename based on model and alpha
qhat_filename = f"{percentage}perc_{s}_{epochs}epochs_alpha{alpha}.txt"

# Save qhat value to a file
output_path = os.path.join(f"/content/drive/MyDrive/Forth/OMBRIA/trained_models_S1GF/", qhat_filename)
with open(output_path, "w") as f:
    f.write(str(qhat))

print(f"qhat value saved to: {output_path}")
"""

"""# With temp scaling"""

cell_code_TS_CP = """
alpha = 0.1
# Obtain predictions from the model
n = np.shape(results)[0]*np.shape(results)[1]*np.shape(results)[2]
print(n)
results = scaled_probabilities#model.predict(test_dataset, verbose=1)
y_pred = np.where(results[:, :, :, 0] > 0.5, 1, 0)

# Flatten predictions and ground truth
y_pred_vector = y_pred.flatten()
results_vector = results.reshape(n)
probability_array = np.column_stack((1 - results_vector, results_vector))
true_mask_vector = np.array(y_true).flatten().astype(int)

# Fixed calibration and validation splits
cal_pred = probability_array
cal_labels = true_mask_vector

# Conformal Prediction calculations
# Calculate conformal scores for the calibration set
cal_scores = 1 - cal_pred[np.arange(n), cal_labels]

# Calculate quantile and threshold
q_level = np.ceil((n + 1) * (1 - alpha)) / n
qhat = np.quantile(cal_scores, q_level, interpolation='higher')


# Define output filename based on model and alpha
qhat_filename = f"TS_{s}_{epochs}epochs_alpha{alpha}.txt"

# Save qhat value to a file
output_path = os.path.join(f"/content/drive/MyDrive/Forth/OMBRIA/trained_models_split_{percentage}/{s}/", qhat_filename)
with open(output_path, "w") as f:
    f.write(str(qhat))

print(f"qhat value saved to: {output_path}")
"""

"""# Master Run"""

percentages = [80]
modalities = ['S2']
epochs_ = [20]#[10, 20, 50]
alphas = [0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05]

for s in modalities:
  for percentage in percentages:
    for epochs in epochs_:
      saved_model_path = f"/content/drive/MyDrive/Forth/OMBRIA/trained_models_split_{percentage}/{s}/FloodNet_model_{epochs}epochs.keras"
      model = load_model(saved_model_path, custom_objects={'LeakyReLU': LeakyReLU})
      if s == 'S1':
        test_path = f'/content/drive/MyDrive/Forth/OMBRIA/Split_{percentage}/s1/reserved'
        exec(cell_code_s1)
      elif s == 'S2':
        test_path = f'/content/drive/MyDrive/Forth/OMBRIA/Split_{percentage}/s2/reserved'
        exec(cell_code_s2)
      elif s == 'S1_S2':
        test_path = f'/content/drive/MyDrive/Forth/OMBRIA/Split_{percentage}/s2/reserved'
        test_path1 = f'/content/drive/MyDrive/Forth/OMBRIA/Split_{percentage}/s2/reserved'
        test_path2 = f'/content/drive/MyDrive/Forth/OMBRIA/Split_{percentage}/s1/reserved'
        exec(cell_code_s1_s2)
      for alpha_ in alphas:
        exec(cell_code_rest)
        exec(cell_code_CP)
        #exec(cell_code_TS1)
        #exec(cell_code_TS2)
        #exec(cell_code_TS3)
        #exec(cell_code_TS4)
        #exec(cell_code_TS_CP)

percentages = [80, 90]
modalities = ['S2', 'S1_S2']
epochs_ = [10, 20, 50]


for s in modalities:
  for percentage in percentages:
    for epochs in epochs_:
      saved_model_path = f"/content/drive/MyDrive/Forth/OMBRIA/trained_models_split_{percentage}/{s}/FloodNet_model_{epochs}epochs.keras"
      model = load_model(saved_model_path, custom_objects={'LeakyReLU': LeakyReLU})
      if s == 'S1':
        test_path = f'/content/drive/MyDrive/Forth/OMBRIA/Split_{percentage}/s1/reserved'
        exec(cell_code_s1)
      elif s == 'S2':
        test_path = f'/content/drive/MyDrive/Forth/OMBRIA/Split_{percentage}/s2/reserved'
        exec(cell_code_s2)
      elif s == 'S1_S2':
        test_path = f'/content/drive/MyDrive/Forth/OMBRIA/Split_{percentage}/s2/reserved'
        test_path1 = f'/content/drive/MyDrive/Forth/OMBRIA/Split_{percentage}/s2/reserved'
        test_path2 = f'/content/drive/MyDrive/Forth/OMBRIA/Split_{percentage}/s1/reserved'
        exec(cell_code_s1_s2)
      exec(cell_code_rest)
      exec(cell_code_CP)
      exec(cell_code_TS1)
      exec(cell_code_TS2)
      exec(cell_code_TS3)
      exec(cell_code_TS4)
      exec(cell_code_TS_CP)

"""# Master Run Other Datasets"""

percentages = [95] #[90,95]
epochs_ = [20]#[10, 20, 50]
alphas = [0.01] #[0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05]
s = 'S1'

for percentage in percentages:
  for epochs in epochs_:
    saved_model_path = f"/content/drive/MyDrive/Forth/OMBRIA/trained_models_Split_95_S1GF/FloodNet_model_{epochs}epochs.keras"
    model = load_model(saved_model_path, custom_objects={'LeakyReLU': LeakyReLU})

    test_path = f'/content/drive/MyDrive/Forth/OMBRIA/S1GFloods/val'
    exec(cell_code_s1)
    for alpha_ in alphas:
      exec(cell_code_rest)
      exec(cell_code_CP)
      #exec(cell_code_TS1)
      #exec(cell_code_TS2)
      #exec(cell_code_TS3)
      #exec(cell_code_TS4)
      #exec(cell_code_TS_CP)

import matplotlib.pyplot as plt
print(cal_scores.shape)
# If results is not yet flattened
cal_vector = cal_scores.flatten()

# Plot histogram
plt.figure(figsize=(8, 5))
plt.hist(cal_vector, bins=50, color='lightgreen', edgecolor='black')
plt.xlabel("Model Prediction")
plt.ylabel("Frequency")
plt.title("Histogram of Model Predictions on Test Set")
plt.grid(True)
plt.tight_layout()
plt.show()

percentages = [95] #[90,95]
epochs_ = [20]#[10, 20, 50]
alphas = [0.01] #[0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05]
s = 'S1'
for percentage in percentages:
  for epochs in epochs_:
    saved_model_path = f"/content/drive/MyDrive/Forth/OMBRIA/trained_models_Split_{percentage}_S1GF/FloodNet_model_{epochs}epochs.keras"
    model = load_model(saved_model_path, custom_objects={'LeakyReLU': LeakyReLU})

    test_path = f'/content/drive/MyDrive/Forth/OMBRIA/S1GFloods_Split_{percentage}/reserved'
    exec(cell_code_s1)
    for alpha_ in alphas:
      exec(cell_code_rest)
      exec(cell_code_CP)
      #exec(cell_code_TS1)
      #exec(cell_code_TS2)
      #exec(cell_code_TS3)
      #exec(cell_code_TS4)
      #exec(cell_code_TS_CP)